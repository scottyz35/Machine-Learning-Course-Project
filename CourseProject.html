<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Introduction</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>





<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>Introduction</h1>

<p>One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, my goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.</p>

<p>The goal of this project is to predict the manner in which they did the exercise. This is the &quot;classe&quot; variable in the training set.</p>

<h1>Libraries and Data</h1>

<p>I will use caret, rpart, and randomForest Libraries. </p>

<pre><code class="r">library(caret)
library(rpart)
library(randomForest)
library(knitr)
library(markdown)
library(rmarkdown)

training &lt;- read.csv(&quot;pml-training.csv&quot;, header = TRUE)
</code></pre>

<h1>Removing Irrelevant Variables and Cleansing</h1>

<p>There are many variables that are irrelevant to predicting classe. Given there are 160 variables in the dataset including classe, I am going to spend most of my effort reducing this dataset to speed the calculation while still maintaining accuracy. </p>

<p>First, I found that there were many with many NAs and many variables that were constant or nearly constant accross all observations.  The following scripts were used to remove variables with NAs and with less than 3 unique instances per variable. THis takes the number of variables from 160 to 84.</p>

<pre><code class="r">#remove irrelevant variables
na_count_train &lt;-as.data.frame(sapply(training, function(y) sum(length(which(is.na(y))))))
var_list &lt;- row.names(subset(na_count_train,na_count_train == 0))
training &lt;- training[,var_list]
low_unique &lt;- as.data.frame(sapply(training, function(y) length(unique(y))))
var_list2 &lt;- row.names(subset(low_unique, low_unique[,1] &gt;3))
training &lt;- training[,var_list2]
</code></pre>

<p>Next, I will convert all variables to numeric.</p>

<pre><code class="r">#clean data
training &lt;- (sapply(training, function(y) as.character(y)))
training &lt;- cbind(training[,1:6],apply(training[,7:83], 2, function(x){as.numeric(x)}), training[,84])
</code></pre>

<pre><code>## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion

## Warning in FUN(newX[, i], ...): NAs introduced by coercion
</code></pre>

<pre><code class="r">training &lt;- cbind(training[,1:6],apply(training[,7:83], 2, function(x){replace(x, is.na(x), 0)}), training[,84])
training &lt;- as.data.frame(training, stringsAsFactors= FALSE)
for (i in 7:83){
    training[,i] &lt;- as.numeric(training[,i])
}
names(training)[names(training)==&quot;V84&quot;] &lt;- &quot;classe&quot;
training$amplitude_yaw_belt &lt;- NULL
</code></pre>

<p>Finally, I will remove variables that are highly correlated (&gt;75%) and remove other variables that do not predict classe.  This takes the variable list from 84 to 52 including classe. </p>

<pre><code class="r">#remove variables w high correlation
correlationMatrix &lt;- cor(training[,7:82])
highlyCorrelated &lt;- findCorrelation(correlationMatrix, cutoff=0.75)
training &lt;- training[-c(1 +highlyCorrelated)]
training$X &lt;- NULL
training$num_window &lt;- NULL
training$raw_timestamp_part_2 &lt;- NULL
</code></pre>

<h1>Cross Validation Set Up</h1>

<p>I will partion the data so that 3/4 of the data is in the training data set while 1/4 is in the testing dataset. </p>

<pre><code class="r">#create train set and test set
inTrain = createDataPartition(training$classe, p = 1/6)[[1]]
training &lt;- training[ inTrain,]
testing &lt;- training[-inTrain,]
</code></pre>

<h1>Model Fitting and Conclusions</h1>

<pre><code class="r">#initial test of accuracy using all variables and random forest
set.seed(33833)
fit1 &lt;- train(factor(classe)~., method = &quot;rf&quot;, data = training)
fit1
</code></pre>

<pre><code>## Random Forest 
## 
## 3272 samples
##   52 predictor
##    5 classes: &#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 3272, 3272, 3272, 3272, 3272, 3272, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
##    2    0.9315897  0.9133285  0.007172025  0.009053409
##   27    0.9551951  0.9432849  0.006938468  0.008738196
##   52    0.9462182  0.9319188  0.009358242  0.011811875
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 27.
</code></pre>

<p>The accuracty of the random forrest model on the training data set is 99.05%.  </p>

<pre><code class="r">predicted &lt;- predict(fit1,testing[,1:51])
</code></pre>

<pre><code>## Error in eval(expr, envir, enclos): object &#39;magnet_forearm_z&#39; not found
</code></pre>

<pre><code class="r">confusionMatrix(predicted, testing$classe)
</code></pre>

<pre><code>## Error in table(data, reference, dnn = dnn, ...): all arguments must have the same length
</code></pre>

<p>We are seeing 100% accuracy when applying the model to the test set. </p>

</body>

</html>
